[
  {
    "objectID": "methodology.html",
    "href": "methodology.html",
    "title": "Survey Methodology and Information",
    "section": "",
    "text": "This page contains detailed information about the methodology used for our survey and analysis, including:\n\nOur objectives and research questions\nOur recruitment and cognitive pilot\nSurvey structure and content\nDemographics of respondees\n\nOur aims in presenting this information is to a) ensure transparency and reproducibility, and b) enable others to reuse the codebase for similar work in the future.\n\n\nAs noted in the about page, this study was conducted by the Innovation & Impact Hub (part of the Turing Research and Innovation cluster for Digital Twins) as part of the TEA-DT project.\nThe aim of this survey was to explore the challenges and opportunities for assurance of digital twins within the community, with a particular focus on the application of high-level guiding principles such as the Gemini Principles.\n\n\n\n\n\n\nAbout the Community\n\n\n\nOur goal was to target digital twin “practitioners”, broadly construed as those actively involved in research, development, or governance of digital twins. This could include those who were part of a project to build a digital twin, or those working on tools or techniques for digital twinning.\n\n\nThe study was divided into three main topics and for each topic we defined a set of research questions:\n\nTopic 1: Current understanding and practices in assuring digital twins.\n\nRQ1: What is the maturity of the surveyed digital twin community?\nRQ2: What is the community’s current understanding of assurance?\nRQ3: What practices are being used, including methods and properties assured?\n\nTopic 2: Attitudes & perceived challenges in putting ethical and trustworthy digital twin principles into practice.\n\nRQ4: How satisfied is the community with their assurance practices?\nRQ5: How does the community perceive guiding principles (eg Gemini Principles)?\nRQ6: What challenges do people encounter when putting the Gemini Principles into practice?\n\ntopic 3: Readiness for, and attitudes towards, new tools for argument-based assurance.\n\nRQ7: What is the community’s readiness for argument-based assurance methods?\nRQ8: What support is needed for successful adoption?\n\n\n\n\n\nThe survey aimed to gather insights from digital twin practitioners and was distributed to a number of digital twinning communities, including the Digital Twin Hub’s community (comprising industry, academia, and public sector), the DTnet+ community (an academic network of digital twin practitioners), and the broader Alan Turing Institute’s university network. Participants were recruited through community newsletters and social media (Twitter & LinkedIn). The survey was conducted completely anonymous, and we did not collect any sensitive data by default. Respondents could choose to submit their email address for the specific purpose of being contacted for events related to this piece of work (e.g. workshops presenting our analysis).\nWe received a total of 50 responses, representing a broad spectrum of roles, including senior & strategic leadership, technical specialists, and research positions (see demographics). The responses were roughly evenly split between these categories. Most participants reported balancing a range of responsibilities, including technical decision-making and operational management, while only four individuals included “Compliance” in their primary responsibilities.\n\n\nA cognitive pilot was conducted with four digital twin practitioners to evaluate the survey’s design and relevance. Participants filled in the survey in a live call while sharing their screen and vocalizing their thoughts. This allowed for immediate feedback on question clarity and response options. A combination of interviewer observation and retrospective probing techniques were used to identify potential problems of the survey and probe understanding. For example, in case of hesitation, the respondents were prompted to report on the possible reason (eg whether the question was not relevant enough to the respondents, ambiguous in its meaning or response options were not representative enough). At the end, feedback was collected on the survey’s length, value of dynamic results, and overall relevance.\nThe feedback revealed that while the survey was engaging and appropriately balanced between multiple choice and free-text responses, there were concerns about limited response options and ambiguous wording in some questions. Based on these insights, changes were made to improve question phrasing, add more detailed response options, introduce a question about the primary purpose of digital twins, and provide additional help fields to aid navigation.\n\n\n\n\nFollowing the adjustments made from the cognitive pilot, the survey was divided into five sections:\n\nBackground Information (e.g. sector, location)\nCurrent Assurance Practices and Understanding\nSatisfaction with Assurance Practices\nHigh-Level Assurance Goals and Ethical Frameworks\nCommunicating Assurance\n\nThe entire survey consisted of 25 core questions, with an additional eight questions that were conditional on previous responses (see example below). In addition, participants were asked to rate each Gemini Principle on how relevant and how challenging it was. The level of challenge, however, was only requested for those principles that were not deemed irrelevant. As a result, the number of questions varied considerably depending on the responses given throughout.\n\n\n\n\n\n\nList of Questions\n\n\n\nTo see a list of all survey questions, please click here.\n\n\n\n\nThe survey began in on May 30th and was open until July 31st.\nThe survey was developed as a web-based application using the open-source Python framework Streamlit, enabling an interactive and user-friendly experience for participants. It was hosted on Azure infrastructure in line with our ethics review to data protection and secure data handling.\nThe survey took an average of 18 minutes to complete. Throughout the survey, respondents received live feedback on some of the aggregated answers that had been submitted so far, making the experience more engaging.\n\n\n\n\n\n\nConditional Question Example\n\n\n\nIn the survey, some questions were dependent on participants’ previous responses.\n\nMain Question: “Has your organisation established one or more digital twins?” (NB: this question was asked to all participants.)\nFollow-up Question (Conditional): “What type of digital twin(s)?” (NB: this follow-up question was only shown to participants who answered “yes” or “indirectly” to the main question.)\n\n\n\n\n\n\nFor data analysis, we primarily used descriptive statistics to summarize quantitative responses. Additionally, we performed a more detailed content analysis on the qualitative data collected from four open-ended questions, with coding conducted by three independent coders to ensure reliability and depth of insights (see our Analysis section for further details).\n\n\n\n\nRespondents came from ten different countries, with the majority based in the UK, and some selecting “global” as the primary location for their company.\n\n\n\nTable 1: Number of responses by location.\n\n\n\n\n\n\n\n\n\nLocation\nCount\n\n\n\n\nUnited Kingdom\n33\n\n\nGlobal\n8\n\n\nUnited States\n2\n\n\nFrance\n1\n\n\nGermany\n1\n\n\nItaly\n1\n\n\nJapan\n1\n\n\nSingapore\n1\n\n\nSouth Africa\n1\n\n\nUnited Arab Emirates\n1\n\n\n\n\n\n\n Survey participants represented over 19 different sectors, with larger groups emerging from Energy, IT, and Engineering sectors:\n\n\n\nTable 2: Number of responses by sector.\n\n\n\n\n\n\n\n\n\nSector\nCount\n\n\n\n\nEnergy\n7\n\n\nInformation technology / Software\n5\n\n\nEngineering\n5\n\n\nHealthcare\n3\n\n\nSmart Cities\n3\n\n\nOther\n3\n\n\nManufacturing\n3\n\n\nArtificial Intelligence\n3\n\n\nEducation\n3\n\n\nDefence\n3\n\n\nConstruction\n2\n\n\nTechnology\n2\n\n\nTransport\n2\n\n\nEnvironment and Conservation\n1\n\n\nAviation\n1\n\n\nNational Government\n1\n\n\nPlace Leadership\n1\n\n\nTelecommunications\n1\n\n\nWater\n1\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Distribution of Survey Respondents by Role: The pie chart shows the percentage of survey respondents based on their roles within their organizations. The question asked was, “What is your role within your organisation?”\n\n\n\n\n\n\nOne limitation of this survey is the bias towards participants from the UK. This bias is largely due to the networks through which the survey was distributed, which have a predominantly UK-based focus. Additionally, as the survey was conducted anonymously, we were unable to ensure balanced representation across other demographics. This anonymity, while protecting respondent privacy, limits our ability to assess and correct for potential imbalances in the survey sample."
  },
  {
    "objectID": "methodology.html#study-objectives-research-questions",
    "href": "methodology.html#study-objectives-research-questions",
    "title": "Survey Methodology and Information",
    "section": "",
    "text": "As noted in the about page, this study was conducted by the Innovation & Impact Hub (part of the Turing Research and Innovation cluster for Digital Twins) as part of the TEA-DT project.\nThe aim of this survey was to explore the challenges and opportunities for assurance of digital twins within the community, with a particular focus on the application of high-level guiding principles such as the Gemini Principles.\n\n\n\n\n\n\nAbout the Community\n\n\n\nOur goal was to target digital twin “practitioners”, broadly construed as those actively involved in research, development, or governance of digital twins. This could include those who were part of a project to build a digital twin, or those working on tools or techniques for digital twinning.\n\n\nThe study was divided into three main topics and for each topic we defined a set of research questions:\n\nTopic 1: Current understanding and practices in assuring digital twins.\n\nRQ1: What is the maturity of the surveyed digital twin community?\nRQ2: What is the community’s current understanding of assurance?\nRQ3: What practices are being used, including methods and properties assured?\n\nTopic 2: Attitudes & perceived challenges in putting ethical and trustworthy digital twin principles into practice.\n\nRQ4: How satisfied is the community with their assurance practices?\nRQ5: How does the community perceive guiding principles (eg Gemini Principles)?\nRQ6: What challenges do people encounter when putting the Gemini Principles into practice?\n\ntopic 3: Readiness for, and attitudes towards, new tools for argument-based assurance.\n\nRQ7: What is the community’s readiness for argument-based assurance methods?\nRQ8: What support is needed for successful adoption?"
  },
  {
    "objectID": "methodology.html#survey-sample-and-recruitment",
    "href": "methodology.html#survey-sample-and-recruitment",
    "title": "Survey Methodology and Information",
    "section": "",
    "text": "The survey aimed to gather insights from digital twin practitioners and was distributed to a number of digital twinning communities, including the Digital Twin Hub’s community (comprising industry, academia, and public sector), the DTnet+ community (an academic network of digital twin practitioners), and the broader Alan Turing Institute’s university network. Participants were recruited through community newsletters and social media (Twitter & LinkedIn). The survey was conducted completely anonymous, and we did not collect any sensitive data by default. Respondents could choose to submit their email address for the specific purpose of being contacted for events related to this piece of work (e.g. workshops presenting our analysis).\nWe received a total of 50 responses, representing a broad spectrum of roles, including senior & strategic leadership, technical specialists, and research positions (see demographics). The responses were roughly evenly split between these categories. Most participants reported balancing a range of responsibilities, including technical decision-making and operational management, while only four individuals included “Compliance” in their primary responsibilities.\n\n\nA cognitive pilot was conducted with four digital twin practitioners to evaluate the survey’s design and relevance. Participants filled in the survey in a live call while sharing their screen and vocalizing their thoughts. This allowed for immediate feedback on question clarity and response options. A combination of interviewer observation and retrospective probing techniques were used to identify potential problems of the survey and probe understanding. For example, in case of hesitation, the respondents were prompted to report on the possible reason (eg whether the question was not relevant enough to the respondents, ambiguous in its meaning or response options were not representative enough). At the end, feedback was collected on the survey’s length, value of dynamic results, and overall relevance.\nThe feedback revealed that while the survey was engaging and appropriately balanced between multiple choice and free-text responses, there were concerns about limited response options and ambiguous wording in some questions. Based on these insights, changes were made to improve question phrasing, add more detailed response options, introduce a question about the primary purpose of digital twins, and provide additional help fields to aid navigation."
  },
  {
    "objectID": "methodology.html#survey-structure",
    "href": "methodology.html#survey-structure",
    "title": "Survey Methodology and Information",
    "section": "",
    "text": "Following the adjustments made from the cognitive pilot, the survey was divided into five sections:\n\nBackground Information (e.g. sector, location)\nCurrent Assurance Practices and Understanding\nSatisfaction with Assurance Practices\nHigh-Level Assurance Goals and Ethical Frameworks\nCommunicating Assurance\n\nThe entire survey consisted of 25 core questions, with an additional eight questions that were conditional on previous responses (see example below). In addition, participants were asked to rate each Gemini Principle on how relevant and how challenging it was. The level of challenge, however, was only requested for those principles that were not deemed irrelevant. As a result, the number of questions varied considerably depending on the responses given throughout.\n\n\n\n\n\n\nList of Questions\n\n\n\nTo see a list of all survey questions, please click here.\n\n\n\n\nThe survey began in on May 30th and was open until July 31st.\nThe survey was developed as a web-based application using the open-source Python framework Streamlit, enabling an interactive and user-friendly experience for participants. It was hosted on Azure infrastructure in line with our ethics review to data protection and secure data handling.\nThe survey took an average of 18 minutes to complete. Throughout the survey, respondents received live feedback on some of the aggregated answers that had been submitted so far, making the experience more engaging.\n\n\n\n\n\n\nConditional Question Example\n\n\n\nIn the survey, some questions were dependent on participants’ previous responses.\n\nMain Question: “Has your organisation established one or more digital twins?” (NB: this question was asked to all participants.)\nFollow-up Question (Conditional): “What type of digital twin(s)?” (NB: this follow-up question was only shown to participants who answered “yes” or “indirectly” to the main question.)\n\n\n\n\n\n\nFor data analysis, we primarily used descriptive statistics to summarize quantitative responses. Additionally, we performed a more detailed content analysis on the qualitative data collected from four open-ended questions, with coding conducted by three independent coders to ensure reliability and depth of insights (see our Analysis section for further details)."
  },
  {
    "objectID": "methodology.html#demographics",
    "href": "methodology.html#demographics",
    "title": "Survey Methodology and Information",
    "section": "",
    "text": "Respondents came from ten different countries, with the majority based in the UK, and some selecting “global” as the primary location for their company.\n\n\n\nTable 1: Number of responses by location.\n\n\n\n\n\n\n\n\n\nLocation\nCount\n\n\n\n\nUnited Kingdom\n33\n\n\nGlobal\n8\n\n\nUnited States\n2\n\n\nFrance\n1\n\n\nGermany\n1\n\n\nItaly\n1\n\n\nJapan\n1\n\n\nSingapore\n1\n\n\nSouth Africa\n1\n\n\nUnited Arab Emirates\n1\n\n\n\n\n\n\n Survey participants represented over 19 different sectors, with larger groups emerging from Energy, IT, and Engineering sectors:\n\n\n\nTable 2: Number of responses by sector.\n\n\n\n\n\n\n\n\n\nSector\nCount\n\n\n\n\nEnergy\n7\n\n\nInformation technology / Software\n5\n\n\nEngineering\n5\n\n\nHealthcare\n3\n\n\nSmart Cities\n3\n\n\nOther\n3\n\n\nManufacturing\n3\n\n\nArtificial Intelligence\n3\n\n\nEducation\n3\n\n\nDefence\n3\n\n\nConstruction\n2\n\n\nTechnology\n2\n\n\nTransport\n2\n\n\nEnvironment and Conservation\n1\n\n\nAviation\n1\n\n\nNational Government\n1\n\n\nPlace Leadership\n1\n\n\nTelecommunications\n1\n\n\nWater\n1\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Distribution of Survey Respondents by Role: The pie chart shows the percentage of survey respondents based on their roles within their organizations. The question asked was, “What is your role within your organisation?”"
  },
  {
    "objectID": "methodology.html#limitations",
    "href": "methodology.html#limitations",
    "title": "Survey Methodology and Information",
    "section": "",
    "text": "One limitation of this survey is the bias towards participants from the UK. This bias is largely due to the networks through which the survey was distributed, which have a predominantly UK-based focus. Additionally, as the survey was conducted anonymously, we were unable to ensure balanced representation across other demographics. This anonymity, while protecting respondent privacy, limits our ability to assess and correct for potential imbalances in the survey sample."
  },
  {
    "objectID": "images/report_plots/formatted_plots/properties_assured_caption.html",
    "href": "images/report_plots/formatted_plots/properties_assured_caption.html",
    "title": "TEA-DT Community Pulse Check",
    "section": "",
    "text": "Figure X: Properties Considered When Assuring Digital Twinning Technology. This chart presents the results of the multiple-choice question, “Which of the following properties (or goals) do you currently consider when assuring your (or your client’s) digital twinning technology?” Respondents could select multiple options, and the bars are grouped into categories for readability: Technical Performance, Economic, Societal, Ethical, Legal & Regulatory, and Ecosystem Integration. Each category lists specific principles below the bar in the order corresponding to the stacked segments. The smallest sections represent additional principles specified by respondents who selected “Other.” Note that these groupings are not the only possible categorizations and are used here to enhance the visualization."
  },
  {
    "objectID": "images/report_plots/formatted_plots/quadrant_gemini_caption.html",
    "href": "images/report_plots/formatted_plots/quadrant_gemini_caption.html",
    "title": "TEA-DT Community Pulse Check",
    "section": "",
    "text": "Figure X: Relative Perceptions of Gemini Principles. This scatterplot compares the nine Gemini principles based on their average ratings across two dimensions: relevance and challenge. Each dot represents the average response for a principle across all survey responses, with the x-axis showing the level of challenge (increasing from left to right) and the y-axis showing the level of relevance (increasing from bottom to top). The principles are color-coded for identification: security (dark purple), quality (lighter purple), value (pink), insight (dark blue), public good (orange), federation (yellow), evolution (green/yellow), curation (teal), and openness (light blue). The crosshair marks the midpoints between the most extreme average values, providing a reference for how principles compare to one another."
  },
  {
    "objectID": "images/report_plots/experience_caption.html",
    "href": "images/report_plots/experience_caption.html",
    "title": "TEA-DT Community Pulse Check",
    "section": "",
    "text": "Figure X: Assurance Providers Within Organizations. The bar chart shows the distribution of responses to the multiple-choice question, “Who provides assurance within/for your organisation?” Respondents could choose from four options."
  },
  {
    "objectID": "images/report_plots/comms_caption.html",
    "href": "images/report_plots/comms_caption.html",
    "title": "TEA-DT Community Pulse Check",
    "section": "",
    "text": "Figure X: Methods of Communicating Assurance. This bar chart illustrates the responses to the question, “How do you currently communicate your assurance strategies to stakeholders, partner organisations, or your clients?” Respondents could choose multiple options from a predefined list, and the percentages indicate the proportion of participants who selected each method."
  },
  {
    "objectID": "images/report_plots/relevance_stacked_caption.html",
    "href": "images/report_plots/relevance_stacked_caption.html",
    "title": "TEA-DT Community Pulse Check",
    "section": "",
    "text": "Figure X: Gemini Principles Sorted by Relevance. The bar chart shows how respondents rated the relevance of various Gemini principles in response to the question, “Please rate, for each of the following Gemini principles, the extent to which it focuses on issues that you believe to be relevant for your work.”. Ratings were provided on a scale from “Extremely Relevant” to “Not Relevant,” with green shades representing higher relevance (Extremely, Very) and yellow shades representing lower relevance (Moderately Relevant, Slightly Relevant and Not Relevant). The stacked bars indicate the percentage of respondents who selected each level of relevance for each principle. The principles are sorted by the total number of responses indicating high relevance (green shades), with those receiving more high relevance ratings appearing at the top."
  },
  {
    "objectID": "images/report_plots/challenge_stacked_caption.html",
    "href": "images/report_plots/challenge_stacked_caption.html",
    "title": "TEA-DT Community Pulse Check",
    "section": "",
    "text": "Figure X: Gemini Principles Sorted by Difficulty. The bar chart shows how respondents rated the difficulty of various Gemini principles in response to the question, “Please rate, for each of the following Gemini principles: How challenging is it to define and/or to know how to currently address it in practices?”. Ratings were provided on a scale from “Extremely Challenging” to “Not at all Challenging,” with blue shades representing higher amounts of challenge (Extremely, Very) and purple shades representing lower amounts of challenge (Slightly Challenging and Not at all Challenging). The stacked bars indicate the percentage of respondents who selected each level of challenge for each principle. The principles are sorted by the total number of responses indicating high amounts of challenge (blue shades), with those receiving more challenging ratings appearing at the top. The amount of responses for “Moderately challenging” are not displayed, however, across all principles this was the most frequently selected option."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis of Survey Results",
    "section": "",
    "text": "In this section you will find our main analysis of the results gathered from the community pulse check. The analysis is presented across the following sections:\n\nCurrent Practices and Attitudes\nEvaluating the Gemini Principles\nReadines for Argument-Based Assurance\n\n\n\n\n\n\n\nOther Resources\n\n\n\nThe analysis presented on this page is only a selection of the key results. However, other resources are available:\n\nTo view our survey questions, please click here.\nTo view the code used for this analysis (e.g. production of graphs), please click here.\nTo view the thematic content analaysis of the free-text answers, please click here.\n\n\n\n\n\nFor this project, it was important to have a high-level understanding of the current assurance practices and attitudes within the DT community, alongside basic information about the community composition (e.g. sector) to help segment the findings. The findings in this section adress these topics.\n\n\nAn unsurprising first finding (given the nature of the survey and its tageted community) is that 86% of respondents reported that they have established a digital twin, either directly or by supporting clients or providing components for digital twins. Despite the obvious selection bias, this high level of adoption indicates a mature and growing involvement with digital twin technology across the surveyed community.\nHowever, the lack of a shared definition of what constitutes a digital twin means that respondents may vary in how conservative or liberal their interpretation is, potentially inflating the reported percentage. A recent report by the Committee on Foundational Research Gaps and Future Directions for Digital Twins et al. (2024) provides a comprehensive and expansive definition of digital twins, which may help build consensus, but this remains an open issue at present.\n\n\n\nThe majority of respondents (68%) currently conduct assurance internally, with most relying on non-specialized teams for this process (see Figure 1). Only 6% of respondents use external services, such as third-party assurance providers, while 18% of respondents themselves are the provider of assurance services. This suggests a strong preference for in-house assurance, though often with limited specialisation.\nSupply of assurance tools and techniques by third-party providers has also been raised recently in a report published by the UK Government’s Department for Science, Innovation, and Technology Department for Science (2024), suggesting that there could be a change in these practices as more diverse options become available.\n\n\n\n\n\n\nFigure 1: Assurance Providers Within Organizations. The bar chart shows the distribution of responses to the multiple-choice question, “Who provides assurance within/for your organisation?” Respondents could choose from four options.\n\n\n\n\n\n\nIn our previous work, we had noted that a common goal for assurance practices was the safety or security of a system or technology Burr et al. (2024). However, in conducting this survey we were interested in whether assurance methods such as argument-based assurance (e.g. TEA, GSN) could help operationalise a broader set of principles—specifically the Gemini principles. We also wanted to see which goals or properties the community were currently assuring for digital twins.\nAs shown in Figure 2, respondents assure approximately seven or fewer properties of digital twins, focusing primarily on technical and economic performance. Societal properties, such as safety and trustworthiness, are assured to a lesser extent, with ethical, legal, and regulatory aspects being even less frequently addressed. Ecosystem integration properties, except for interoperability, were the least selected, highlighting gaps in broader assurance considerations.\n\n\n\n\n\n\nFigure 2: Properties Considered When Assuring Digital Twinning Technology. This chart presents the results of the multiple-choice question, “Which of the following properties (or goals) do you currently consider when assuring your (or your client’s) digital twinning technology?” Respondents could select multiple options, and the bars are grouped into categories for readability: Technical Performance, Economic, Societal, Ethical, Legal & Regulatory, and Ecosystem Integration. Each category lists specific principles below the bar in the order corresponding to the stacked segments. The smallest sections represent additional principles specified by respondents who selected “Other.” Note that these groupings are not the only possible categorisations and are used here to enhance the visualisation.\n\n\n\n\n\n\nThe understanding of the concept ‘assurance’ varied widely among respondents, as seen in the wordcloud depicted in Figure 3. Some focused on specific properties of digital twins (i.e. a specific target of assurance), while others emphasised the broader goal of increasing trust (i.e. the general goal of the process of assurance). We observed key themes around trust and confidence, as well as mention of validation, verification, or testing, For example:\n\n“We ensure a digital twin is having the impact anticipated and operating effectively.”\n“DT has been designed in accordance with requirements and is fulfilling them.”\n“Continuous validation and verification against real-world conditions.”\n“Verification and validation that the data is as intended.”\n“Independent validation of transparency, security, and trustworthiness in the data, processes, and purpose of the digital twin.”\n\nSome responses mentioned specific goals such as accuracy, utility, privacy, and reliability. Notably, safety and compliance were less prominent. The diversity in responses indicates that assurance, as a structured process or methodology (e.g. argument-based assurance), is not yet a mature or consistently shared concept across the digital twin community.\n\n\n\n\n\n\nFigure 3: Definition of Assurance for Digital Twinning. This word cloud illustrates the themes identified during the thematic analysis of the free-text responses to the question, “What do you understand ‘assurance’ to mean in the context of your work in the digital twinning sector?”. The size of each term reflects the frequency with which the theme occurred, with larger terms appearing more frequently. The color corresponds to the size of the terms, providing a visual cue for the prominence of different themes.\n\n\n\n\n\n\nTo explore how a new platform or approach to assurance (e.g. the TEA platform) would be perceived by the DT community, it was important to first understand the community’s attitudes towards their current assurance practices. As such, respondents were asked how satisfied they were with how their team identifies and documents requirements, actions, and decisions in their assurance process (see bottom bar of Figure 4). We also provided three statements related to assurance integration, communication, and alignment with higher-level principles:\n\n“The way we communicate our assurance activities significantly contributes to building and maintaining trust and confidence among our (or our client’s) stakeholders.”\n“We can clearly link specific assurance activities directly to higher-level principles guiding our (or our client’s) system’s trustworthiness and ethical standards.”\n“Our assurance activities extend beyond mere checklist compliance and are substantively integrated into our (or our client’s) operational processes.”\n\nRespondents were asked to rate their level of agreement on a Likert scale from 1 (strongly disagree) to 5 (strongly agree). As we can see in the top three bars of Figure 4, respondents reported a high level of satisfaction with their assurance activities, particularly in the effectiveness of communicating assurance to build stakeholder trust. The majority also agreed that assurance activities are substantively integrated into operational practices and that these activities can be clearly linked to higher-level trustworthiness and ethical principles.\nHowever, there was also a notable minority who disagreed with these statements, indicating variability in how assurance practices are perceived and implemented. This divergence may partly explain why 24% of respondents reported being unsatisfied with their overall assurance process.\n\n\n\n\n\n\nFigure 4: Attitudes and Satisfaction Towards Assurance Practices. This figure presents the results from multiple questions assessing respondents’ attitudes and satisfaction with assurance practices. The first three bars correspond to statements where respondents rated their level of agreement on a Likert scale from disagreeing (orange) to agreeing (green). The stacked bars show the percentage of respondents selecting each Likert rating, with colors coding for the type of response.The bottom bar reflects responses to the question, “How satisfied are you with how your team identifies and documents requirements, actions, and decisions in your assurance process?” Ratings ranged from very unsatisfied (purple) to very satisfied (blues). Percentages of “Neutral” responses are represented in grey (not displayed as bars).\n\n\n\n\n\n\n\nIn addition to the current practices and attitudes, a further key motivation for this survey was to better understand the use and adoption of the Gemini Principles. Specifically, we wanted to understand how the community assessed the utility of these principles and whether there was operational value from the perspective of assurance.\n\n\n\n\n\n\nWhat are the Gemini Principles?\n\n\n\nThe Gemini Principles, developed by the Centre for Digital Built Britain (CDBB), serve as a guiding framework for the development and implementation of digital twin initiatives across various sectors and domains. Born from a collaborative effort involving a diverse array of experts spanning various sectors, including academia, industry, and government, the Gemini Principles embody a collective vision for the future of digital infrastructure in the UK. Their creation was a strategic initiative designed to ensure that the evolution of digital twins and the broader digital built environment aligns with public interests, fosters innovation, and upholds the highest ethical standards.\n\n\n\nThe Gemini Principles. Reprinted from https://digitaltwinhub.co.uk/download/the-gemini-principles/\n\n\n\n\n\n\nRespondents were first asked about their familiarity with, use of, and the perceived relevance of these principles. 64% of respondents had some level of familiarity with the principles, with 30% of this group actively using them as guidelines.1 The remaining respondents either had limited knowledge (i.e. “seen but not used”) or were introduced to the principles for the first time through the survey (Figure 5).\n\n\n\n\n\n\nFigure 5: Familiarity with the Gemini Principles. The bar chart illustrates the distribution of respondents’ familiarity with the Gemini Principles. Participants were asked, “How familiar are you with the Gemini principles?” and selected one of the response options. The bars represent the percentage of respondents selecting each level of familiarity, showing a range from those who have never encountered the principles to those who actively use or contributed to their development.\n\n\n\nWhen asked to rate the relevance of each principle, most respondents indicated that they found them “very” or “extremely relevant,” showing strong overall agreement on their importance. However, when assessing their practical value, responses were more mixed, with many considering the principles only “moderately valuable.” One may think that this suggests that while the principles are recognised as the right set of guidelines, they are not perceived as valuable in practice. However, it is important to note here that principles are insufficient to specify actions or practical decisions. Rather, principles are intended to play a contributory (but vital) role in a process of reflection and deliberation. As such, they are best viewed as starting points for a participatory approach of decision-making, rather than expecting them to serve as procedures for identifying practical actions or steps.\n\n\n\nHigh relevance overall\n\n\n\n\n\nModerate Value\n\n\n\n\n\nAll of the Gemini principles were rated as either “Very” or “Extremely Relevant” by the majority of respondents, indicating a broad consensus on their overall importance. Among these, “Insight,” “Value,” and “Quality” were more often rated as “extremely relevant” compared to other principles, reflecting their particular significance within the community.\nWhile this is a useful finding in its own right, but becomes more actionable when we consider it alonsgide the next finding.\n\n\n\n\n\n\nFigure 6: Gemini Principles Sorted by Relevance. The bar chart shows how respondents rated the relevance of various Gemini principles in response to the question, “Please rate, for each of the following Gemini principles, the extent to which it focuses on issues that you believe to be relevant for your work.”. Ratings were provided on a scale from “Extremely Relevant” to “Not Relevant,” with green shades representing higher relevance (Extremely, Very) and yellow shades representing lower relevance (Moderately Relevant, Slightly Relevant and Not Relevant). The stacked bars indicate the percentage of respondents who selected each level of relevance for each principle. The principles are sorted by the total number of responses indicating high relevance (green shades), with those receiving more high relevance ratings appearing at the top.\n\n\n\n\n\n\nFor all principles (except those rated as irrelevant), respondents were then asked to rate how challenging each principle was to implement. While “Moderately Challenging” was the most common response across all principles ==(not displayed below!)==, notable differences emerged in the more extreme ratings. For example, “federation” received the highest number of “extremely challenging” ratings, followed by “public good,”evolution” and “security.” In contrast, “insight” was the only principle never rated as “extremely challenging” and was generally seen as less challenging. Interestingly, the principles of “curation” and “openness” had symmetric extreme responses, with equal numbers finding them either not at all challenging or extremely challenging, possibly reflecting differences in sector or stages of digital twin adoption. This variability highlights the complexity of putting these principles into practice and warrants further exploration through in-depth interviews to understand the underlying reasons.\n\n\n\n\n\n\nFigure 7: Gemini Principles Sorted by Difficulty. The bar chart shows how respondents rated the difficulty of various Gemini principles in response to the question, “Please rate, for each of the following Gemini principles: How challenging is it to define and/or to know how to currently address it in practices?”. Ratings were provided on a scale from “Extremely Challenging” to “Not at all Challenging,” with blue shades representing higher amounts of challenge (Extremely, Very) and purple shades representing lower amounts of challenge (Slightly Challenging and Not at all Challenging). The stacked bars indicate the percentage of respondents who selected each level of challenge for each principle. The principles are sorted by the total number of responses indicating high amounts of challenge (blue shades), with those receiving more challenging ratings appearing at the top. The amount of responses for “Moderately challenging” are not displayed, however, across all principles this was the most frequently selected option.\n\n\n\nCombining these responses with the results from the previous findings allows us to compare the principles along these two dimensions, as shown in Figure 8. Here it is easier to see that “quality” and “security”, relatively speaking, are possibly the most impactful to assure (or provide research support towards), given their high relevance and current perceived challenge in the community. In other words, these two principles are good candidates to be worked out further, as most practitioners would benefit from more specific and illustrative best practices in those areas.\n\n\n\n\n\n\nFigure 8: Relative Perceptions of Gemini Principles. This scatterplot compares the nine Gemini principles based on their average ratings across two dimensions: relevance and challenge. Each dot represents the average response for a principle across all survey responses, with the x-axis showing the level of challenge (increasing from left to right) and the y-axis showing the level of relevance (increasing from bottom to top). The principles are color-coded for identification: security (dark purple), quality (lighter purple), value (pink), insight (dark blue), public good (orange), federation (yellow), evolution (green/yellow), curation (teal), and openness (light blue). The crosshair marks the midpoints between the most extreme average values, providing a reference for how principles compare to one another.\n\n\n\n\n\n\nOnly about half (55%) of respondents reported considering sharing data or models with other organisations to build connected digital twins, which may explain why the “Federation” principle received relatively low relevance ratings. This suggests that not all digital twin practitioners are focused on creating connected digital twins (Bennett et al. 2023). Among those who did pursue connected twins, 67% found it difficult to establish trust in the resulting shared digital twin. The challenges reported included a broad range of issues, with the most common being intellectual property rights, data confidentiality, interoperability, and insufficient digital awareness. This indicates that for those who find “Federation” relevant, it is often perceived as highly challenging to implement.\n\n\n\n\n\n\nFigure 9: Data Sharing and Perceived Trust Challenges. This sunburst plot illustrates the relevance of data sharing within the digital twin community and the perceived difficulty in establishing trust when sharing data. The inner pie chart represents the ratio of responses to the question, “Have you considered sharing data or models with other organisations (or across partners within an organisation) to form connected digital twins?” with 55% answering “Yes” (blue) and 45% answering “No” (purple). For those who responded “Yes” a follow-up question asked about the difficulty of establishing trust when partnering to build connected digital twins. Respondents could choose from six options, which were later consolidated into three categories: “Easy”, “Neutral”, and “Difficult”. The light blue outer ring displays the percentage of these categories across all responses.\n\n\n\n\n\n\n\n\n\nFigure 10: Challenges in Building Connected Digital Twins. The bar chart presents the results of a multiple-choice question where respondents were asked to identify the major challenges to overcome when building connected digital twins. This question was conditional on respondents having answered that they do consider sharing data/models. Participants could select multiple options from a list of pre-defined challenges, and the bars show the percentage of respondents who selected each challenge. The challenges are color-coded to suggest potential categories. While these categories help group related challenges, they are not intended to be definitive or exhaustive.\n\n\n\n\n\n\nThe main challenges reported when putting the Gemini Principles into practice were categorised into four key themes:\n\nNot knowing how\nCommunicating value\nData Concerns\nLack of consistency\n\n\n\n\nWhat do you understand ‘assurance’ to mean?\n\n\nThe theme “Not knowing how” best encapsulates a sense of uncertainty among respondents, often reflecting an inability to determine where to begin or a lack of the necessary knowledge, including the absence of appropriate metrics or KPIs. This theme was especially prominent for principles like “Public good,” “Evolution,” and “Value,” highlighting that the digital twin space is still developing and that some sectors exhibit lower maturity when addressing these foundational concepts. For instance: - “It is not clear what public good in perpetuity would look like.” - “It is hard to objectively measure public good.” - “Terms are not well enough defined in our context.”\n“Data Concerns” and “Lack of consistency” often co-occurred, encompassing issues ranging from specific data quality problems to broader challenges like the absence of consistent standards across the community. Examples include:\n- “The acquisition of data on which digital twins are typically built is generally not standard in medical practice.”\n- “Lack of standards and frameworks for data exchange among physical digital twins.”\nFinally, the challenge of “Communicating value” was often linked to principles such as “Value,” “Openness,” and “Federation.” Respondents referred to the difficulty of establishing a shared understanding of a digital twin’s value among multiple stakeholders. Examples include:\n- “Being able to communicate the value to their end user.”\n- “Interoperability is a challenge (…) including getting partners to understand the opportunity and benefit at a system of systems level.”\nThese challenges demonstrate that while the Gemini Principles are recognized as important, there are still significant barriers to their implementation, with many sectors struggling to apply these high-level concepts effectively.\n\n\n\n\n\n\nWhen asked about their readiness for a tool to support argument-based assurance, 48% of respondents indicated that such a tool would enhance trust in their digital twins by helping structure and communicate how assurance measures align with ethical goals. Another 38% expressed interest but required more information to fully understand its utility, while only 14% did not see the value of such a tool. Among those interested, common benefits mentioned were the potential for a more standardized approach, improved understanding of both the system and assurance process, and the ability to challenge or revisit assurance arguments. In contrast, those who were not interested in the tool often cited that it was not applicable to their work or did not align with existing communication practices with their stakeholders.\n\n\n\nA significant gap was identified in the foundational understanding of digital twin ethics, with 72% of respondents indicating that their organizations lacked an established definition or framework for trustworthy and ethical digital twins. This highlights a lack of clarity in operationalizing ethical principles.\n\n\n\nframework\n\n\n\n\n\nThe majority of respondents (64%) communicate their assurance work verbally during meetings, with 42% not using any structured approach for this communication. Only 30% use visual aids in their reports, demonstrating a clear gap in effectively structuring and conveying assurance cases across the industry. Although some respondents do follow established standards or use visual aids, further exploration is needed to better understand current practices and identify how TEA could support and integrate with these existing approaches.\n\n\n\n\n\n\nFigure 11: Methods of Communicating Assurance. This bar chart illustrates the responses to the question, “How do you currently communicate your assurance strategies to stakeholders, partner organisations, or your clients?” Respondents could choose multiple options from a predefined list, and the percentages indicate the proportion of participants who selected each method.\n\n\n\n\n\n\nWhen asked about the support needed to adopt the TEA tool and create assurance cases around ethical principles, the majority of respondents indicated that they would require a value demonstration to justify building an assurance case, emphasizing the need for a clear business justification. Additionally, a significant number of respondents highlighted their lack of expertise in implementing assurance tools as a major challenge, making skills training the second most requested form of support. A substantial number of respondents selected costs and lack of time as potential challenges, although those were chosen less frequently. The least frequently selected challenges were around internal resistance and integration with governance process or tech stack. Interestingly, community forums were the least requested support option, chosen by only 24% of respondents, suggesting a preference for more direct or formal methods of learning and engagement."
  },
  {
    "objectID": "analysis.html#current-practices-and-attitudes",
    "href": "analysis.html#current-practices-and-attitudes",
    "title": "Analysis of Survey Results",
    "section": "",
    "text": "For this project, it was important to have a high-level understanding of the current assurance practices and attitudes within the DT community, alongside basic information about the community composition (e.g. sector) to help segment the findings. The findings in this section adress these topics.\n\n\nAn unsurprising first finding (given the nature of the survey and its tageted community) is that 86% of respondents reported that they have established a digital twin, either directly or by supporting clients or providing components for digital twins. Despite the obvious selection bias, this high level of adoption indicates a mature and growing involvement with digital twin technology across the surveyed community.\nHowever, the lack of a shared definition of what constitutes a digital twin means that respondents may vary in how conservative or liberal their interpretation is, potentially inflating the reported percentage. A recent report by the Committee on Foundational Research Gaps and Future Directions for Digital Twins et al. (2024) provides a comprehensive and expansive definition of digital twins, which may help build consensus, but this remains an open issue at present.\n\n\n\nThe majority of respondents (68%) currently conduct assurance internally, with most relying on non-specialized teams for this process (see Figure 1). Only 6% of respondents use external services, such as third-party assurance providers, while 18% of respondents themselves are the provider of assurance services. This suggests a strong preference for in-house assurance, though often with limited specialisation.\nSupply of assurance tools and techniques by third-party providers has also been raised recently in a report published by the UK Government’s Department for Science, Innovation, and Technology Department for Science (2024), suggesting that there could be a change in these practices as more diverse options become available.\n\n\n\n\n\n\nFigure 1: Assurance Providers Within Organizations. The bar chart shows the distribution of responses to the multiple-choice question, “Who provides assurance within/for your organisation?” Respondents could choose from four options.\n\n\n\n\n\n\nIn our previous work, we had noted that a common goal for assurance practices was the safety or security of a system or technology Burr et al. (2024). However, in conducting this survey we were interested in whether assurance methods such as argument-based assurance (e.g. TEA, GSN) could help operationalise a broader set of principles—specifically the Gemini principles. We also wanted to see which goals or properties the community were currently assuring for digital twins.\nAs shown in Figure 2, respondents assure approximately seven or fewer properties of digital twins, focusing primarily on technical and economic performance. Societal properties, such as safety and trustworthiness, are assured to a lesser extent, with ethical, legal, and regulatory aspects being even less frequently addressed. Ecosystem integration properties, except for interoperability, were the least selected, highlighting gaps in broader assurance considerations.\n\n\n\n\n\n\nFigure 2: Properties Considered When Assuring Digital Twinning Technology. This chart presents the results of the multiple-choice question, “Which of the following properties (or goals) do you currently consider when assuring your (or your client’s) digital twinning technology?” Respondents could select multiple options, and the bars are grouped into categories for readability: Technical Performance, Economic, Societal, Ethical, Legal & Regulatory, and Ecosystem Integration. Each category lists specific principles below the bar in the order corresponding to the stacked segments. The smallest sections represent additional principles specified by respondents who selected “Other.” Note that these groupings are not the only possible categorisations and are used here to enhance the visualisation.\n\n\n\n\n\n\nThe understanding of the concept ‘assurance’ varied widely among respondents, as seen in the wordcloud depicted in Figure 3. Some focused on specific properties of digital twins (i.e. a specific target of assurance), while others emphasised the broader goal of increasing trust (i.e. the general goal of the process of assurance). We observed key themes around trust and confidence, as well as mention of validation, verification, or testing, For example:\n\n“We ensure a digital twin is having the impact anticipated and operating effectively.”\n“DT has been designed in accordance with requirements and is fulfilling them.”\n“Continuous validation and verification against real-world conditions.”\n“Verification and validation that the data is as intended.”\n“Independent validation of transparency, security, and trustworthiness in the data, processes, and purpose of the digital twin.”\n\nSome responses mentioned specific goals such as accuracy, utility, privacy, and reliability. Notably, safety and compliance were less prominent. The diversity in responses indicates that assurance, as a structured process or methodology (e.g. argument-based assurance), is not yet a mature or consistently shared concept across the digital twin community.\n\n\n\n\n\n\nFigure 3: Definition of Assurance for Digital Twinning. This word cloud illustrates the themes identified during the thematic analysis of the free-text responses to the question, “What do you understand ‘assurance’ to mean in the context of your work in the digital twinning sector?”. The size of each term reflects the frequency with which the theme occurred, with larger terms appearing more frequently. The color corresponds to the size of the terms, providing a visual cue for the prominence of different themes.\n\n\n\n\n\n\nTo explore how a new platform or approach to assurance (e.g. the TEA platform) would be perceived by the DT community, it was important to first understand the community’s attitudes towards their current assurance practices. As such, respondents were asked how satisfied they were with how their team identifies and documents requirements, actions, and decisions in their assurance process (see bottom bar of Figure 4). We also provided three statements related to assurance integration, communication, and alignment with higher-level principles:\n\n“The way we communicate our assurance activities significantly contributes to building and maintaining trust and confidence among our (or our client’s) stakeholders.”\n“We can clearly link specific assurance activities directly to higher-level principles guiding our (or our client’s) system’s trustworthiness and ethical standards.”\n“Our assurance activities extend beyond mere checklist compliance and are substantively integrated into our (or our client’s) operational processes.”\n\nRespondents were asked to rate their level of agreement on a Likert scale from 1 (strongly disagree) to 5 (strongly agree). As we can see in the top three bars of Figure 4, respondents reported a high level of satisfaction with their assurance activities, particularly in the effectiveness of communicating assurance to build stakeholder trust. The majority also agreed that assurance activities are substantively integrated into operational practices and that these activities can be clearly linked to higher-level trustworthiness and ethical principles.\nHowever, there was also a notable minority who disagreed with these statements, indicating variability in how assurance practices are perceived and implemented. This divergence may partly explain why 24% of respondents reported being unsatisfied with their overall assurance process.\n\n\n\n\n\n\nFigure 4: Attitudes and Satisfaction Towards Assurance Practices. This figure presents the results from multiple questions assessing respondents’ attitudes and satisfaction with assurance practices. The first three bars correspond to statements where respondents rated their level of agreement on a Likert scale from disagreeing (orange) to agreeing (green). The stacked bars show the percentage of respondents selecting each Likert rating, with colors coding for the type of response.The bottom bar reflects responses to the question, “How satisfied are you with how your team identifies and documents requirements, actions, and decisions in your assurance process?” Ratings ranged from very unsatisfied (purple) to very satisfied (blues). Percentages of “Neutral” responses are represented in grey (not displayed as bars)."
  },
  {
    "objectID": "analysis.html#evaluating-the-gemini-principles",
    "href": "analysis.html#evaluating-the-gemini-principles",
    "title": "Analysis of Survey Results",
    "section": "",
    "text": "In addition to the current practices and attitudes, a further key motivation for this survey was to better understand the use and adoption of the Gemini Principles. Specifically, we wanted to understand how the community assessed the utility of these principles and whether there was operational value from the perspective of assurance.\n\n\n\n\n\n\nWhat are the Gemini Principles?\n\n\n\nThe Gemini Principles, developed by the Centre for Digital Built Britain (CDBB), serve as a guiding framework for the development and implementation of digital twin initiatives across various sectors and domains. Born from a collaborative effort involving a diverse array of experts spanning various sectors, including academia, industry, and government, the Gemini Principles embody a collective vision for the future of digital infrastructure in the UK. Their creation was a strategic initiative designed to ensure that the evolution of digital twins and the broader digital built environment aligns with public interests, fosters innovation, and upholds the highest ethical standards.\n\n\n\nThe Gemini Principles. Reprinted from https://digitaltwinhub.co.uk/download/the-gemini-principles/\n\n\n\n\n\n\nRespondents were first asked about their familiarity with, use of, and the perceived relevance of these principles. 64% of respondents had some level of familiarity with the principles, with 30% of this group actively using them as guidelines.1 The remaining respondents either had limited knowledge (i.e. “seen but not used”) or were introduced to the principles for the first time through the survey (Figure 5).\n\n\n\n\n\n\nFigure 5: Familiarity with the Gemini Principles. The bar chart illustrates the distribution of respondents’ familiarity with the Gemini Principles. Participants were asked, “How familiar are you with the Gemini principles?” and selected one of the response options. The bars represent the percentage of respondents selecting each level of familiarity, showing a range from those who have never encountered the principles to those who actively use or contributed to their development.\n\n\n\nWhen asked to rate the relevance of each principle, most respondents indicated that they found them “very” or “extremely relevant,” showing strong overall agreement on their importance. However, when assessing their practical value, responses were more mixed, with many considering the principles only “moderately valuable.” One may think that this suggests that while the principles are recognised as the right set of guidelines, they are not perceived as valuable in practice. However, it is important to note here that principles are insufficient to specify actions or practical decisions. Rather, principles are intended to play a contributory (but vital) role in a process of reflection and deliberation. As such, they are best viewed as starting points for a participatory approach of decision-making, rather than expecting them to serve as procedures for identifying practical actions or steps.\n\n\n\nHigh relevance overall\n\n\n\n\n\nModerate Value\n\n\n\n\n\nAll of the Gemini principles were rated as either “Very” or “Extremely Relevant” by the majority of respondents, indicating a broad consensus on their overall importance. Among these, “Insight,” “Value,” and “Quality” were more often rated as “extremely relevant” compared to other principles, reflecting their particular significance within the community.\nWhile this is a useful finding in its own right, but becomes more actionable when we consider it alonsgide the next finding.\n\n\n\n\n\n\nFigure 6: Gemini Principles Sorted by Relevance. The bar chart shows how respondents rated the relevance of various Gemini principles in response to the question, “Please rate, for each of the following Gemini principles, the extent to which it focuses on issues that you believe to be relevant for your work.”. Ratings were provided on a scale from “Extremely Relevant” to “Not Relevant,” with green shades representing higher relevance (Extremely, Very) and yellow shades representing lower relevance (Moderately Relevant, Slightly Relevant and Not Relevant). The stacked bars indicate the percentage of respondents who selected each level of relevance for each principle. The principles are sorted by the total number of responses indicating high relevance (green shades), with those receiving more high relevance ratings appearing at the top.\n\n\n\n\n\n\nFor all principles (except those rated as irrelevant), respondents were then asked to rate how challenging each principle was to implement. While “Moderately Challenging” was the most common response across all principles ==(not displayed below!)==, notable differences emerged in the more extreme ratings. For example, “federation” received the highest number of “extremely challenging” ratings, followed by “public good,”evolution” and “security.” In contrast, “insight” was the only principle never rated as “extremely challenging” and was generally seen as less challenging. Interestingly, the principles of “curation” and “openness” had symmetric extreme responses, with equal numbers finding them either not at all challenging or extremely challenging, possibly reflecting differences in sector or stages of digital twin adoption. This variability highlights the complexity of putting these principles into practice and warrants further exploration through in-depth interviews to understand the underlying reasons.\n\n\n\n\n\n\nFigure 7: Gemini Principles Sorted by Difficulty. The bar chart shows how respondents rated the difficulty of various Gemini principles in response to the question, “Please rate, for each of the following Gemini principles: How challenging is it to define and/or to know how to currently address it in practices?”. Ratings were provided on a scale from “Extremely Challenging” to “Not at all Challenging,” with blue shades representing higher amounts of challenge (Extremely, Very) and purple shades representing lower amounts of challenge (Slightly Challenging and Not at all Challenging). The stacked bars indicate the percentage of respondents who selected each level of challenge for each principle. The principles are sorted by the total number of responses indicating high amounts of challenge (blue shades), with those receiving more challenging ratings appearing at the top. The amount of responses for “Moderately challenging” are not displayed, however, across all principles this was the most frequently selected option.\n\n\n\nCombining these responses with the results from the previous findings allows us to compare the principles along these two dimensions, as shown in Figure 8. Here it is easier to see that “quality” and “security”, relatively speaking, are possibly the most impactful to assure (or provide research support towards), given their high relevance and current perceived challenge in the community. In other words, these two principles are good candidates to be worked out further, as most practitioners would benefit from more specific and illustrative best practices in those areas.\n\n\n\n\n\n\nFigure 8: Relative Perceptions of Gemini Principles. This scatterplot compares the nine Gemini principles based on their average ratings across two dimensions: relevance and challenge. Each dot represents the average response for a principle across all survey responses, with the x-axis showing the level of challenge (increasing from left to right) and the y-axis showing the level of relevance (increasing from bottom to top). The principles are color-coded for identification: security (dark purple), quality (lighter purple), value (pink), insight (dark blue), public good (orange), federation (yellow), evolution (green/yellow), curation (teal), and openness (light blue). The crosshair marks the midpoints between the most extreme average values, providing a reference for how principles compare to one another.\n\n\n\n\n\n\nOnly about half (55%) of respondents reported considering sharing data or models with other organisations to build connected digital twins, which may explain why the “Federation” principle received relatively low relevance ratings. This suggests that not all digital twin practitioners are focused on creating connected digital twins (Bennett et al. 2023). Among those who did pursue connected twins, 67% found it difficult to establish trust in the resulting shared digital twin. The challenges reported included a broad range of issues, with the most common being intellectual property rights, data confidentiality, interoperability, and insufficient digital awareness. This indicates that for those who find “Federation” relevant, it is often perceived as highly challenging to implement.\n\n\n\n\n\n\nFigure 9: Data Sharing and Perceived Trust Challenges. This sunburst plot illustrates the relevance of data sharing within the digital twin community and the perceived difficulty in establishing trust when sharing data. The inner pie chart represents the ratio of responses to the question, “Have you considered sharing data or models with other organisations (or across partners within an organisation) to form connected digital twins?” with 55% answering “Yes” (blue) and 45% answering “No” (purple). For those who responded “Yes” a follow-up question asked about the difficulty of establishing trust when partnering to build connected digital twins. Respondents could choose from six options, which were later consolidated into three categories: “Easy”, “Neutral”, and “Difficult”. The light blue outer ring displays the percentage of these categories across all responses.\n\n\n\n\n\n\n\n\n\nFigure 10: Challenges in Building Connected Digital Twins. The bar chart presents the results of a multiple-choice question where respondents were asked to identify the major challenges to overcome when building connected digital twins. This question was conditional on respondents having answered that they do consider sharing data/models. Participants could select multiple options from a list of pre-defined challenges, and the bars show the percentage of respondents who selected each challenge. The challenges are color-coded to suggest potential categories. While these categories help group related challenges, they are not intended to be definitive or exhaustive.\n\n\n\n\n\n\nThe main challenges reported when putting the Gemini Principles into practice were categorised into four key themes:\n\nNot knowing how\nCommunicating value\nData Concerns\nLack of consistency\n\n\n\n\nWhat do you understand ‘assurance’ to mean?\n\n\nThe theme “Not knowing how” best encapsulates a sense of uncertainty among respondents, often reflecting an inability to determine where to begin or a lack of the necessary knowledge, including the absence of appropriate metrics or KPIs. This theme was especially prominent for principles like “Public good,” “Evolution,” and “Value,” highlighting that the digital twin space is still developing and that some sectors exhibit lower maturity when addressing these foundational concepts. For instance: - “It is not clear what public good in perpetuity would look like.” - “It is hard to objectively measure public good.” - “Terms are not well enough defined in our context.”\n“Data Concerns” and “Lack of consistency” often co-occurred, encompassing issues ranging from specific data quality problems to broader challenges like the absence of consistent standards across the community. Examples include:\n- “The acquisition of data on which digital twins are typically built is generally not standard in medical practice.”\n- “Lack of standards and frameworks for data exchange among physical digital twins.”\nFinally, the challenge of “Communicating value” was often linked to principles such as “Value,” “Openness,” and “Federation.” Respondents referred to the difficulty of establishing a shared understanding of a digital twin’s value among multiple stakeholders. Examples include:\n- “Being able to communicate the value to their end user.”\n- “Interoperability is a challenge (…) including getting partners to understand the opportunity and benefit at a system of systems level.”\nThese challenges demonstrate that while the Gemini Principles are recognized as important, there are still significant barriers to their implementation, with many sectors struggling to apply these high-level concepts effectively."
  },
  {
    "objectID": "analysis.html#readiness-for-argument-based-assurance",
    "href": "analysis.html#readiness-for-argument-based-assurance",
    "title": "Analysis of Survey Results",
    "section": "",
    "text": "When asked about their readiness for a tool to support argument-based assurance, 48% of respondents indicated that such a tool would enhance trust in their digital twins by helping structure and communicate how assurance measures align with ethical goals. Another 38% expressed interest but required more information to fully understand its utility, while only 14% did not see the value of such a tool. Among those interested, common benefits mentioned were the potential for a more standardized approach, improved understanding of both the system and assurance process, and the ability to challenge or revisit assurance arguments. In contrast, those who were not interested in the tool often cited that it was not applicable to their work or did not align with existing communication practices with their stakeholders.\n\n\n\nA significant gap was identified in the foundational understanding of digital twin ethics, with 72% of respondents indicating that their organizations lacked an established definition or framework for trustworthy and ethical digital twins. This highlights a lack of clarity in operationalizing ethical principles.\n\n\n\nframework\n\n\n\n\n\nThe majority of respondents (64%) communicate their assurance work verbally during meetings, with 42% not using any structured approach for this communication. Only 30% use visual aids in their reports, demonstrating a clear gap in effectively structuring and conveying assurance cases across the industry. Although some respondents do follow established standards or use visual aids, further exploration is needed to better understand current practices and identify how TEA could support and integrate with these existing approaches.\n\n\n\n\n\n\nFigure 11: Methods of Communicating Assurance. This bar chart illustrates the responses to the question, “How do you currently communicate your assurance strategies to stakeholders, partner organisations, or your clients?” Respondents could choose multiple options from a predefined list, and the percentages indicate the proportion of participants who selected each method.\n\n\n\n\n\n\nWhen asked about the support needed to adopt the TEA tool and create assurance cases around ethical principles, the majority of respondents indicated that they would require a value demonstration to justify building an assurance case, emphasizing the need for a clear business justification. Additionally, a significant number of respondents highlighted their lack of expertise in implementing assurance tools as a major challenge, making skills training the second most requested form of support. A substantial number of respondents selected costs and lack of time as potential challenges, although those were chosen less frequently. The least frequently selected challenges were around internal resistance and integration with governance process or tech stack. Interestingly, community forums were the least requested support option, chosen by only 24% of respondents, suggesting a preference for more direct or formal methods of learning and engagement."
  },
  {
    "objectID": "analysis.html#footnotes",
    "href": "analysis.html#footnotes",
    "title": "Analysis of Survey Results",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt is unclear how the Gemini principles are used as guidelines, as this was not directly asked during the survey or in any targeted follow-up.↩︎"
  },
  {
    "objectID": "notebooks.html",
    "href": "notebooks.html",
    "title": "Notebooks",
    "section": "",
    "text": "Notebooks\n\n\n\n\n\n\nNote\n\n\n\nOn this page you will find a list and description of our Jupyter notebooks (available on our GitHub repository), which present and explain the code used during different stages of our analysis.\n\n\n\n\n\nNotebook Title\nDescription\n\n\n\n\nNarrative Report\nThis notebook provides the code used for creating the charts and analysis presented in our narrative report.\n\n\nContent Analysis\nThis notebook documents the method we applied for systematically examining the free-text data collected across four distinct survey fields."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site has been setup to help users navigate the resources of this repository.\nOn this site you will find the following:"
  },
  {
    "objectID": "about.html#trustworthy-and-ethical-assurance-of-digital-twins-tea-dt",
    "href": "about.html#trustworthy-and-ethical-assurance-of-digital-twins-tea-dt",
    "title": "About",
    "section": "Trustworthy and Ethical Assurance of Digital Twins (TEA-DT)",
    "text": "Trustworthy and Ethical Assurance of Digital Twins (TEA-DT)\nThe Trustworthy and Ethical Assurance of Digital Twins project ran from February to October 2024. It was a collaborative project between the Turing Research and Innovation Cluster in Digital Twins (Alan Turing Institute) and the Centre for Assuring Autonomy (University of York), with support from the Responsible Technology Adoption Unit (UK Department for Science, Innovation and Technology). Funding for the project was awarded by the BRAID programme (UKRI AHRC).\nThere were three primary objectives for this project:\n\nScoping research into assurance of digital twins\nCo-design of an open-source tool and platform\nEstablish a multi-disciplinary and multi-sector community of practice\n\n\nCommunity Pulse Check\nAs part of the scoping research, Dr Sophie Arana led the design and delivery of a survey, known as our “community pulse check”, to help identify current attitudes, needs, and capabilities, among digital twin practitioners (e.g. researchers and developers).\nThis survey was carried out in partnership with the Digital Twin Hub (Connected Places Catapult) given their extensive network of over 5000 members.\n\n\nProject Team\n\n\n\n\nSophie Arana\n\n\n   Christopher Burr  \n\n\n   Carlos Gavidia-Calderon  \n\n\n   Nathan Hughes  \n\n\n\n\n\n\nAcknowledgements\nIn addition to the above contributors, we would also like to thank the following people:\n\n\n\nAbout our Codebase\nFor the community pulse-check, we designed an interactive form that could provide feedback to participants as soon as they submitted their responses. This was only possible for a subset of questions, such as those with a fixed set of responses:\n\nWhich of the following assurance mechanisms do you currently rely on for your own (or your client’s) digital twin(s)?\n\nThis was achieved by using a free and open-source framework known as Streamlit. Streamlit is designed to help users build data-driven web applications, with an emphasis on interactive dashboards and visualisations, using Python (i.e. with minimal knowledge of HTML, CSS, and Javascript).\nThat said, it took considerable time and effort to customise the survey and ensure that community responses were properly displayed, raising questions among the team about whether it provided sufficient value to those completing the survey.\nHowever, for transparency, our code is fully available through the associated GitHub repository."
  },
  {
    "objectID": "about.html#app-directory",
    "href": "about.html#app-directory",
    "title": "About",
    "section": "app directory",
    "text": "app directory\nIn the app/webapp/ directory, you will find code for the main application, such as the following individual Python files:\n\nwebapp/pages/01_Consent.py: Consent was obtained prior to completion of the survey by requirining explicit acknowledgement of key statements. This file handles the consent form.\nwebapp/pages/02_1_-_Community.py: This file generates survey questions related to the community’s digital twin practices, and is similar to other pages within this directory (e.g. 06_3_-_Satisfaction.py).\nwebapp/pages/03_Community_Results.py: This file displays the results from the questions in the previous page (i.e. 02_1_-_Community.py). It retrieves data from our database (MongoDB), counts responses matching the user’s profile, and provides feedback based on the count.\nwebapp/mongo_utils.py: This file contains utility functions for interacting with database. It includes functions for initialising the database connection, checking response counts, and retrieving field values from the database.\nwebapp/plot_utils.py: This file contains utility functions for generating plots and visualisations.\nMakefile: The Makefile streamlines key development and deployment tasks for the application. It includes commands for building and running the app locally or in Docker, managing dependencies, and setting up a database. Additionally, it provides utilities for code linting, formatting, and type checking to maintain code quality.\n\nThese files, and the remaining files contained within the webapp directory, collectively manage the survey’s flow, from user consent to displaying results and follow-up questions, while interacting with the MongoDB database and generating visualisations using Streamlit."
  },
  {
    "objectID": "questions.html",
    "href": "questions.html",
    "title": "List of Questions",
    "section": "",
    "text": "List of Questions\n\nOn this page you will find a list of the survey’s questions, split into the relevant sections.\n\n\nSection 1: Background Information\n\n\n\n\n\n\n\n\nNumber\nQuestion\nResponse Options\n\n\n\n\nQ1.1\nIn which country is your organisation registered?\nGlobal, United Kingdom, [List of all countries]\n\n\nQ1.2\nWhat is your role within your organisation?\nSenior Management (e.g., CEO, CFO, CTO), Strategic/Business Lead (e.g., Business Unit Head, Project/Program Manager, Operations Manager), Strategic Advisor (e.g., Compliance Officer, Regulatory Affairs Manager, Legal Advisor), In-house Technical Specialist (e.g., Developer/Engineer, Data Scientist/Analyst, IT Specialist), Digital Twin Support Specialist (e.g., Ontology Developer, Dashboard Developer, Data Integration Specialist), Consultant/External Specialist (e.g., Industry Consultant, External IT Consultant, Freelance Technical Expert), Research and Development (e.g., Researcher/Academic, Innovation Specialist, Product Development Scientist), Other (Please specify)\n\n\nQ1.3\nWhat are your primary responsibilities?\nStrategic Direction, Budget Management, Ensuring Compliance, Project Leadership, Technical Decision-Making, Operational Management, Governance Influence, Research and Innovation, Other (Please specify)\n\n\nQ1.4\nWhat sector best represents your field of work?\nAerospace, Architecture, Artificial Intelligence, Automotive, Aviation, Construction, Consumer Goods, Defence, Education, Electronics, Engineering, Environment and Conservation, Finance, Food and Agriculture, Freight, Healthcare, Information technology / Software, International Government, Local Government, Manufacturing, Maritime, Media, Mining, National Government, Energy, Oil and Gas, Place Leadership, Rail, Smart Cities, Supply Chain and Logistics, Technology, Telecommunications, Transport, Utilities, Waste and Recycling, Water, Other\n\n\nQ1.5\nHas your organisation established one or more Digital Twins?\nYes, No, Indirectly (We support clients or provide components for digital twins)\n\n\nQ1.5b\n[If Yes or Indirectly] What type of digital twin? Please Select all that apply\nSystem, Place/Infrastructure Digital Twin, Product, Process, Physical asset, Other (Please specify)\n\n\nQ1.5c\n[If Yes or Indirectly] Which of the following best describes the purpose of the digital twin(s) you have (helped) establish?\nInternal Operations, External Collaboration, Public Impact, Market / Customer Engagement, Research and Development, Other\n\n\nQ1.5d\n[If No] What is the main reason?\nUnclear business case/ROI, insufficient digital awareness/skills, lack of goodwill/competence/data assurance trust in partner organisation, concerns re confidentiality of data, IPR, regulatory concerns re DP, info security, legal agreement barriers, Other\n\n\n\n\n\nSection 2: Current Assurance Practices and Understanding\n\n\n\n\n\n\n\n\nNumber\nQuestion\nResponse Options\n\n\n\n\nQ2.1\nWhat do you understand ‘assurance’ to mean in the context of your work?\nOpen-ended response\n\n\nQ2.2\nWhich of the following assurance mechanisms do you currently rely on?\nBias Reviews, Compliance Audits, Conformity Assessments, Stakeholder Feedback Systems, Impact Assessment, Risk Assessment, Information Security Reviews, Data Quality Checks, Formal Verification, Post-Implementation Evaluation, Service Continuity Management, Performance Monitoring, Operational Audits, Service Quality Reviews, Other (Please specify)\n\n\nQ2.3\n[If Q2.2 not empty] Which of the following properties do you currently consider when assuring your digital twinning technology?\nAccountability, Contestability, Data Quality, Data Stewardship, Ethical Integrity, Evolution, Explainability, Fairness, Federation, Financial Performance, Fit-for-purpose, Governance, Interoperability, Openness, Public Good, Reliability/Robustness, Resilience / Fault-tolerance, Safety, Security, Sustainability, Transparency, Trustworthiness, Value Creation, None, Other (Please Specify)\n\n\nQ2.4\nWho provides assurance within/for your organisation?\nIn-house assurance team, In-house, non-specialized team, External third-party, We provide assurance services for clients\n\n\nQ2.5\nHave you considered sharing your asset-related data or models with other organisations to form connected digital twins?\nYes, No\n\n\nQ2.5b\n[If Yes] When partnering with other organisations for building connected digital twins, how difficult is it to establish trust?\nVery Easy, Somewhat Easy, Neutral, Somewhat Difficult, Very Difficult (+ not applicable)\n\n\nQ2.5c\n[If 2.5b selected as more difficult or neutral than easy] What are/were the major challenges to overcome?\nMultiple Purpose\n\n\n\n\n\nSection 3: Satisfaction with Assurance Practices\n\n\n\n\n\n\n\n\nNumber\nQuestion\nResponse Options\n\n\n\n\nQ3.1\nTo what extent do you agree with the following statements:\nStrongly disagree, Disagree, Neutral, Agree, Strongly agree\n\n\nQ3.2\nHow satisfied are you currently with justification and documentation around your assurance process?\nVery unsatisfied, Somewhat unsatisfied, Neutral, Somewhat satisfied, Very satisfied\n\n\n\n\n\nSection 4: High-Level Assurance Goals and Ethical Frameworks\n\n\n\n\n\n\n\n\nNumber\nQuestion\nResponse Options\n\n\n\n\nQ4.1\nDoes your organisation have an established definition or framework for “trustworthy” and “ethical” digital twins?\nYes, No, I don’t know\n\n\nQ4.2\n[If yes or something similar] If your framework is publicly available, please provide a link URL\nOpen-ended response\n\n\nQ4.3\nHow was this definition or framework developed?\nConsensus-based process, Internal governance process, Developed by an external consultant, Reused/adapted existing framework/standards\n\n\nQ4.4\nHow valuable do you find high-level guiding principles in general?\nNot valuable at all, Slightly valuable, Moderately valuable, Very valuable, Extremely valuable\n\n\nQ4.6\nHow familiar are you with the Gemini principles?\nUnfamiliar, Slightly Familiar, Somewhat Familiar, Familiar, Expert\n\n\nQ4.7\nPlease rate, for each Gemini principle individually, the extent to which it focuses on issues that you believe to be relevant for your work\nNot Relevant, Slightly Relevant, Moderately Relevant, Very Relevant, Extremely Relevant\n\n\nQ4.8\nFor each of the following ethical principles, rate how challenging you find it to determine if you have adequately addressed the principle in practice\nNot challenging at all, Slightly challenging, Moderately challenging, Very challenging, Extremely challenging\n\n\nQ4.9\n[Open-ended] For those rated very difficult or extremely difficult to operationalize, what are the main challenges you face?\nOpen-ended response\n\n\n\n\n\nSection 5: Communicating Assurance\n\n\n\n\n\n\n\n\nNumber\nQuestion\nResponse Options\n\n\n\n\nQ5.1\nHow do you currently communicate your project’s assurance strategies to your stakeholders or partner organisations?\nWritten Reports following established standards, Non-standardized written Reports, Meetings, Visual Aids, Digital Communications, Interactive Platforms, Not Systematically / ad hoc, Other\n\n\nQ5.2\nWould a visual tool that helps you demonstrate and communicate how your evidence-based assurance measures align with key ethical goals enhance trust in your digital twin(s)?\nYes, No, I need to know more about this tool to decide\n\n\nQ5.2b\n[If yes] What do you believe are the main benefits?\nOpen-ended response\n\n\nQ5.2c\n[If no] Why not?\nOpen-ended response\n\n\nQ5.3\nHow prepared do you feel to develop a structured argument for how your current assurance activities relate to broader ethical goals?\nVery prepared, Somewhat prepared, Neutral, Somewhat unprepared, Not prepared at all\n\n\nQ5.4\nWhat would prevent you from adopting a new trustworthy and ethical assurance tool?\nDoesn’t fit into our governance process, Doesn’t integrate into our tech stack, Internal resistance, No time to spend on ethical assurance, Cost prohibitive, Lack of data"
  },
  {
    "objectID": "images/report_plots/wordclouds/wordcloud_meaning_caption.html",
    "href": "images/report_plots/wordclouds/wordcloud_meaning_caption.html",
    "title": "TEA-DT Community Pulse Check",
    "section": "",
    "text": "Figure X: Definition of Assurance for Digital Twinning. This word cloud illustrates the themes identified during the thematic analysis of the free-text responses to the question, “What do you understand ‘assurance’ to mean in the context of your work in the digital twinning sector?”. The size of each term reflects the frequency with which the theme occurred, with larger terms appearing more frequently. The color corresponds to the size of the terms, providing a visual cue for the prominence of different themes."
  },
  {
    "objectID": "images/report_plots/gemini_familiarity_caption.html",
    "href": "images/report_plots/gemini_familiarity_caption.html",
    "title": "TEA-DT Community Pulse Check",
    "section": "",
    "text": "Figure X: Familiarity with the Gemini Principles. The bar chart illustrates the distribution of respondents’ familiarity with the Gemini Principles. Participants were asked, “How familiar are you with the Gemini principles?” and selected one of the response options. The bars represent the percentage of respondents selecting each level of familiarity, showing a range from those who have never encountered the principles to those who actively use or contributed to their development."
  },
  {
    "objectID": "images/report_plots/roles_caption.html",
    "href": "images/report_plots/roles_caption.html",
    "title": "TEA-DT Community Pulse Check",
    "section": "",
    "text": "Figure X: Distribution of Survey Respondents by Role. The pie chart shows the percentage of survey respondents based on their roles within their organizations. The question asked was, “What is your role within your organisation?”"
  },
  {
    "objectID": "images/report_plots/formatted_plots/data_sharing_caption.html",
    "href": "images/report_plots/formatted_plots/data_sharing_caption.html",
    "title": "TEA-DT Community Pulse Check",
    "section": "",
    "text": "Figure X: Data Sharing and Perceived Trust Challenges. This sunburst plot illustrates the relevance of data sharing within the digital twin community and the perceived difficulty in establishing trust when sharing data. The inner pie chart represents the ratio of responses to the question, “Have you considered sharing data or models with other organisations (or across partners within an organisation) to form connected digital twins?” with 55% answering “Yes” (blue) and 45% answering “No” (purple). For those who responded “Yes” a follow-up question asked about the difficulty of establishing trust when partnering to build connected digital twins. Respondents could choose from six options, which were later consolidated into three categories: “Easy” (light blue), “Neutral” (light purple), and “Difficult” (dark blue). The light blue outer ring displays the percentage of these categories across all responses."
  },
  {
    "objectID": "images/report_plots/formatted_plots/attitudes_caption.html",
    "href": "images/report_plots/formatted_plots/attitudes_caption.html",
    "title": "TEA-DT Community Pulse Check",
    "section": "",
    "text": "Figure X: Attitudes and Satisfaction Towards Assurance Practices. This figure presents the results from multiple questions assessing respondents’ attitudes and satisfaction with assurance practices. The first three bars correspond to statements where respondents rated their level of agreement on a Likert scale from disagreeing (orange) to agreeing (green):\n\"The way we communicate our assurance activities significantly contributes to building and maintaining trust and confidence among our (or our client's) stakeholders.\"\n\"We can clearly link specific assurance activities directly to higher-level principles guiding our (or our client's) system's trustworthiness and ethical standards.\"\n\"Our assurance activities extend beyond mere checklist compliance and are substantively integrated into our (or our client's) operational processes.\"\nThe stacked bars show the percentage of respondents selecting each Likert rating, with colors coding for the type of response.The bottom bar reflects responses to the question, “How satisfied are you with how your team identifies and documents requirements, actions, and decisions in your assurance process?” Ratings ranged from very unsatisfied (purple) to very satisfied (blues). Percentages of “Neutral” responses are represented in grey (not displayed as bars)."
  },
  {
    "objectID": "images/report_plots/formatted_plots/trust_challenges_caption.html",
    "href": "images/report_plots/formatted_plots/trust_challenges_caption.html",
    "title": "TEA-DT Community Pulse Check",
    "section": "",
    "text": "Figure X: Challenges in Building Connected Digital Twins. The bar chart presents the results of a multiple-choice question where respondents were asked to identify the major challenges to overcome when building connected digital twins. This question was conditional on respondents having answered that they do consider sharing data/models. Participants could select multiple options from a list of pre-defined challenges, and the bars show the percentage of respondents who selected each challenge. The challenges are color-coded to suggest potential categories. While these categories help group related challenges, they are not intended to be definitive or exhaustive."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TEA-DT Community Pulse Check",
    "section": "",
    "text": "Welcome to our Github repository’s project pages. These pages were set up to help users navigate information pertainining to the TEA-DT Community Pulse Check.\nOn this site you will find (among other things) the following:"
  },
  {
    "objectID": "index.html#news-updates",
    "href": "index.html#news-updates",
    "title": "TEA-DT Community Pulse Check",
    "section": "News & Updates",
    "text": "News & Updates\n\nWe’ll give a brief roundup of the survey results during a DT Hub Gemini Call on Dec 3rd 2024 - Join us!"
  }
]